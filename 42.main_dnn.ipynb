{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Embedding, Input, BatchNormalization, Dropout, concatenate, PReLU, Flatten, Concatenate\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Reshape, Lambda, concatenate, dot, add, RepeatVector\n",
    "from keras.layers import Dropout, GaussianDropout, multiply, SpatialDropout1D, BatchNormalization, subtract\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./out/'):\n",
    "    os.mkdir('./out/')\n",
    "DATA_PATH = '../pkl/'\n",
    "WEIGHT_PATH = './weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data...\n",
      "['iday', 'ihour', 'itime', 'label', 'qid', 'uid', 'invite_answer_gap', 'gender', 'freq', 'A1', 'B1', 'C1', 'D1', 'E1', 'A2', 'B2', 'C2', 'D2', 'E2', 'score', 'num_topic_a', 'num_topic_i', 'most_topic_i', 'min_topic_iv', 'max_topic_iv', 'mean_topic_iv', 'std_topic_iv', 'num_title_sw', 'num_title_w', 'num_desc_sw', 'num_desc_w', 'num_qtopic', 'qhour', 'inv_que_gap', 'num_topic_a_com', 'num_topic_i_com', 'min_topic_iv_com', 'max_topic_iv_com', 'mean_topic_iv_com', 'std_topic_iv_com', 'user_cnt', 'question_cnt', 'question_curr_expo', 'question_history_expo', 'question_future_expo', 'user_curr_expo', 'user_history_expo', 'user_future_expo', 'prev_excellent_sum', 'prev_recommend_sum', 'prev_figure_sum', 'prev_video_sum', 'prev_num_word_sum', 'prev_num_like_sum', 'prev_num_unlike_sum', 'prev_num_comment_sum', 'prev_num_favor_sum', 'prev_num_thank_sum', 'prev_num_report_sum', 'prev_num_nohelp_sum', 'prev_num_oppose_sum', 'prev_cnt_sum', 'prev_excellent_mean', 'prev_recommend_mean', 'prev_figure_mean', 'prev_video_mean', 'prev_num_word_mean', 'prev_num_like_mean', 'prev_num_unlike_mean', 'prev_num_comment_mean', 'prev_num_favor_mean', 'prev_num_thank_mean', 'prev_num_report_mean', 'prev_num_nohelp_mean', 'prev_num_oppose_mean', 'prev_ans_times_min', 'prev_ans_times_mean', 'prev_ans_times_std', 'prev_ans_times_min_gap', 'prev_ans_times_mean_gap', 'qtime_std', 'qtime_mean', 'utime_std', 'utime_mean', 'iweek', 'qlast_itime_gap', 'qllast_itime_gap', 'qlllast_itime_gap', 'qnext_itime_gap', 'qnnext_itime_gap', 'qnnnext_itime_gap', 'ulast_itime_gap', 'ullast_itime_gap', 'ulllast_itime_gap', 'unext_itime_gap', 'unnext_itime_gap', 'unnnext_itime_gap', 'question_history_accept', 'question_history_accept_rate', 'user_history_accept', 'user_history_accept_rate', 'topic_a_sims_min', 'topic_a_sims_max', 'topic_a_sims_mean', 'topic_a_sims_std', 'topic_i_sims_min', 'topic_i_sims_max', 'topic_i_sims_mean', 'topic_i_sims_std', 'prev_topic_sims_min', 'prev_topic_sims_max', 'prev_topic_sims_mean', 'prev_topic_sims_std', 'ques_topic_dnn_enc_A', 'ques_topic_dnn_enc_B', 'ques_topic_dnn_enc_C', 'user_topic_dnn_enc_A', 'user_topic_dnn_enc_B', 'user_topic_dnn_enc_C', 'user_ques_topic_dnn_enc_A', 'user_ques_topic_dnn_enc_B', 'user_ques_topic_dnn_enc_C', 'title_desc_textcnn_enc', 'title_desc_textattbirnn_enc', 'ques_user_title_desc_textattbirnn_enc', 'ques_user_title_desc_textattbirnn_cossim_enc']\n"
     ]
    }
   ],
   "source": [
    "print('load data...')\n",
    "invite_info = pd.read_pickle(os.path.join(DATA_PATH, 'invite_info.pkl'))\n",
    "invite_info_evaluate = pd.read_pickle(os.path.join(DATA_PATH, 'invite_info_evaluate.pkl'))\n",
    "data = pd.read_pickle(os.path.join(DATA_PATH, 'cbt_data.pkl'))\n",
    "print(list(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "['ihour', 'invite_answer_gap', 'gender', 'freq', 'A1', 'B1', 'C1', 'D1', 'E1', 'A2', 'B2', 'C2', 'D2', 'E2', 'score', 'num_topic_a', 'num_topic_i', 'most_topic_i', 'min_topic_iv', 'max_topic_iv', 'mean_topic_iv', 'std_topic_iv', 'num_title_sw', 'num_title_w', 'num_desc_sw', 'num_desc_w', 'num_qtopic', 'qhour', 'inv_que_gap', 'num_topic_a_com', 'num_topic_i_com', 'min_topic_iv_com', 'max_topic_iv_com', 'mean_topic_iv_com', 'std_topic_iv_com', 'user_cnt', 'question_cnt', 'question_curr_expo', 'question_history_expo', 'question_future_expo', 'user_history_expo', 'user_future_expo', 'prev_excellent_sum', 'prev_recommend_sum', 'prev_figure_sum', 'prev_video_sum', 'prev_num_word_sum', 'prev_num_like_sum', 'prev_num_unlike_sum', 'prev_num_comment_sum', 'prev_num_favor_sum', 'prev_num_thank_sum', 'prev_num_report_sum', 'prev_num_nohelp_sum', 'prev_num_oppose_sum', 'prev_cnt_sum', 'prev_excellent_mean', 'prev_recommend_mean', 'prev_figure_mean', 'prev_video_mean', 'prev_num_word_mean', 'prev_num_like_mean', 'prev_num_unlike_mean', 'prev_num_comment_mean', 'prev_num_favor_mean', 'prev_num_thank_mean', 'prev_num_report_mean', 'prev_num_nohelp_mean', 'prev_num_oppose_mean', 'prev_ans_times_min', 'prev_ans_times_mean', 'prev_ans_times_std', 'prev_ans_times_min_gap', 'prev_ans_times_mean_gap', 'qtime_std', 'qtime_mean', 'utime_std', 'utime_mean', 'iweek', 'qlast_itime_gap', 'qllast_itime_gap', 'qlllast_itime_gap', 'qnext_itime_gap', 'qnnext_itime_gap', 'qnnnext_itime_gap', 'ulast_itime_gap', 'ullast_itime_gap', 'ulllast_itime_gap', 'unext_itime_gap', 'unnext_itime_gap', 'unnnext_itime_gap', 'question_history_accept', 'question_history_accept_rate', 'user_history_accept', 'user_history_accept_rate', 'topic_a_sims_min', 'topic_a_sims_max', 'topic_a_sims_mean', 'topic_a_sims_std', 'topic_i_sims_min', 'topic_i_sims_max', 'topic_i_sims_mean', 'topic_i_sims_std', 'prev_topic_sims_min', 'prev_topic_sims_max', 'prev_topic_sims_mean', 'prev_topic_sims_std', 'ques_topic_dnn_enc_A', 'ques_topic_dnn_enc_B', 'ques_topic_dnn_enc_C', 'user_topic_dnn_enc_A', 'user_topic_dnn_enc_B', 'user_topic_dnn_enc_C', 'user_ques_topic_dnn_enc_A', 'user_ques_topic_dnn_enc_B', 'user_ques_topic_dnn_enc_C', 'title_desc_textcnn_enc', 'title_desc_textattbirnn_enc', 'ques_user_title_desc_textattbirnn_enc', 'ques_user_title_desc_textattbirnn_cossim_enc']\n"
     ]
    }
   ],
   "source": [
    "drop_feats = ['qid', 'uid', 'itime', 'label', 'iday', 'user_curr_expo', 'user_curr_expo_d']\n",
    "used_feats = [f for f in data.columns if f not in drop_feats]\n",
    "print(len(used_feats))\n",
    "print(used_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_feat: 13, num_feat: 107\n"
     ]
    }
   ],
   "source": [
    "cat_feat = ['gender', 'freq', 'A1', 'B1', 'C1', 'D1', 'E1', 'A2', 'B2', 'C2', 'D2', 'E2', 'most_topic_i']\n",
    "num_feat = [f for f in used_feats if f not in cat_feat]\n",
    "print('cat_feat: {}, num_feat: {}'.format(len(cat_feat), len(num_feat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:10<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in tqdm(cat_feat):\n",
    "    data[f] = data[f].fillna('-1')\n",
    "    data[f] = LabelEncoder().fit_transform(data[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:20<00:00,  5.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in tqdm(num_feat):\n",
    "    arr = data[f].values\n",
    "    na_idx = np.isnan(arr)\n",
    "    inf_idx = np.isinf(arr)\n",
    "    arr[na_idx] = 0\n",
    "    arr[inf_idx] = 0\n",
    "    max_, min_ = arr.max(), arr.min()\n",
    "    arr = (arr - min_) / (max_ - min_)\n",
    "    arr[na_idx] = -0.1\n",
    "    arr[inf_idx] = 1.1\n",
    "    arr += 0.1\n",
    "    data[f] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(invite_info)\n",
    "train = data[:len_train]\n",
    "test = data[len_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gender', 3),\n",
       " ('freq', 5),\n",
       " ('A1', 2),\n",
       " ('B1', 2),\n",
       " ('C1', 2),\n",
       " ('D1', 2),\n",
       " ('E1', 2),\n",
       " ('A2', 2312),\n",
       " ('B2', 255),\n",
       " ('C2', 400),\n",
       " ('D2', 1356),\n",
       " ('E2', 2),\n",
       " ('most_topic_i', 18406)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_embed_cnt = [(f, data[f].nunique()) for f in cat_feat]\n",
    "single_embed_cnt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train[used_feats].reset_index(drop=True)\n",
    "train_y = train['label'].reset_index(drop=True)\n",
    "test_x = test[used_feats].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1141683, 120)\n",
      "(9489162, 120)\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape)\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def DNN(single_embed_cnt, inp_dense_dim, embed_size=32):\n",
    "    \n",
    "#     # single embedding\n",
    "#     inp_single_embed = []\n",
    "#     out_single_embed = []\n",
    "#     for feat_name, inp_embed_dim in single_embed_cnt:\n",
    "#         inp = Input(shape=(1,))\n",
    "#         inp_single_embed.append(inp)\n",
    "#         x = Embedding(inp_embed_dim, embed_size)(inp)\n",
    "#         x = Flatten()(x)\n",
    "#         out_single_embed.append(x)\n",
    "    \n",
    "#     # dense\n",
    "#     inp_dense = Input(shape=(inp_dense_dim,))\n",
    "#     x = Dense(256)(inp_dense)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = PReLU()(x)\n",
    "#     out_dense = Dropout(0.2)(x)\n",
    "    \n",
    "    \n",
    "#     # concat\n",
    "#     conc = concatenate(out_single_embed + [out_dense])\n",
    "#     conc = Dense(256)(conc)\n",
    "#     conc = BatchNormalization()(conc)\n",
    "#     conc = PReLU()(conc)\n",
    "#     conc = Dropout(0.2)(conc)\n",
    "#     conc = Dense(256)(conc)\n",
    "#     conc = BatchNormalization()(conc)\n",
    "#     conc = PReLU()(conc)\n",
    "#     conc = Dropout(0.2)(conc)\n",
    "#     conc = Dense(256)(conc)\n",
    "#     conc = BatchNormalization()(conc)\n",
    "#     conc = PReLU()(conc)\n",
    "#     conc = Dropout(0.2)(conc)\n",
    "#     out = Dense(1, activation=\"sigmoid\")(conc)\n",
    "#     model = Model(inputs=inp_single_embed+[inp_dense], outputs=out)\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN(single_embed_cnt, inp_dense_dim, embed_size=32):\n",
    "    \n",
    "    # single embedding\n",
    "    inp_single_embed = []\n",
    "    out_single_embed = []\n",
    "    out_lr = []\n",
    "    for feat_name, inp_embed_dim in single_embed_cnt:\n",
    "        inp = Input(shape=(1,))\n",
    "        inp_single_embed.append(inp)\n",
    "        x = Embedding(inp_embed_dim, embed_size)(inp)\n",
    "        x = Flatten()(x)\n",
    "        out_single_embed.append(x)\n",
    "        # \n",
    "        x = Embedding(inp_embed_dim, 1)(inp)\n",
    "        x = Flatten()(x)\n",
    "        out_lr.append(Dense(1)(x))\n",
    "    \n",
    "    # dense\n",
    "    inp_dense = Input(shape=(inp_dense_dim,))\n",
    "    out_lr.append(Dense(1)(inp_dense))\n",
    "    x = Dense(256)(inp_dense)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = PReLU()(x)\n",
    "    out_dense = Dropout(0.2)(x)\n",
    "    \n",
    "    \n",
    "    # concat\n",
    "    conc = concatenate(out_single_embed + [out_dense])\n",
    "    conc = Dense(256)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = PReLU()(conc)\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    conc = Dense(256)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = PReLU()(conc)\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    conc = Dense(1)(conc)\n",
    "    conc = concatenate(out_lr + [conc])\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = PReLU()(conc)\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    out = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    model = Model(inputs=inp_single_embed+[inp_dense], outputs=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, x, y, single_embed_feat, dense_feat, batch_size=128):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.single_embed_feat = single_embed_feat\n",
    "        self.dense_feat = dense_feat\n",
    "        self.batch_size = batch_size\n",
    "#         self.x_single_embed = [x[f].values for f in single_embed_feat]\n",
    "#         self.x_dense = x[dense_feat].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.x.iloc[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "#         batch_x = [batch[f].values for f in self.single_embed_feat] + [batch[f].values for f in self.dense_feat]\n",
    "        batch_x = [batch[f].values for f in self.single_embed_feat] + [batch[self.dense_feat].values]\n",
    "#         batch_x = [xf[idx * self.batch_size:(idx + 1) * self.batch_size] for xf in self.x_single_embed] \\\n",
    "#                 + [self.x_dense[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(Callback):\n",
    "    def __init__(self, x_trn, y_trn, x_val, y_val, cat_feat, num_feat, batch_size=50000, save_name='weight.h5'):\n",
    "        self.trn_generator = DataSequence(x_trn, y_trn, cat_feat, num_feat, BATCH_SIZE)\n",
    "        self.val_generator = DataSequence(x_val, y_val, cat_feat, num_feat, BATCH_SIZE)\n",
    "        self.y_trn = y_trn\n",
    "        self.y_val = y_val\n",
    "        self.save_name = save_name\n",
    "        self.best_score = 0.5\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        y_pred = self.model.predict_generator(self.trn_generator, \n",
    "                                              max_queue_size=10, \n",
    "                                              workers=1, \n",
    "                                              use_multiprocessing=False, \n",
    "                                              verbose=0)\n",
    "        roc = roc_auc_score(self.y_trn, y_pred)\n",
    "        y_pred_val = self.model.predict_generator(self.val_generator, \n",
    "                                              max_queue_size=10, \n",
    "                                              workers=1, \n",
    "                                              use_multiprocessing=False, \n",
    "                                              verbose=0)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n",
    "        \n",
    "        if roc_val > self.best_score:\n",
    "            self.best_score = roc_val\n",
    "            self.model.save_weights(os.path.join(WEIGHT_PATH, self.save_name))\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Fold 0\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "7414/7414 [==============================] - 286s 39ms/step - loss: 0.3723\n",
      "roc-auc: 0.8527 - roc-auc_val: 0.8506                                                                                                    \n",
      "Epoch 2/20\n",
      "7414/7414 [==============================] - 286s 39ms/step - loss: 0.3517\n",
      "roc-auc: 0.8622 - roc-auc_val: 0.8597                                                                                                    \n",
      "Epoch 3/20\n",
      "7414/7414 [==============================] - 209s 28ms/step - loss: 0.3469\n",
      "roc-auc: 0.8686 - roc-auc_val: 0.8633                                                                                                    \n",
      "Epoch 4/20\n",
      "7414/7414 [==============================] - 242s 33ms/step - loss: 0.3437\n",
      "roc-auc: 0.8606 - roc-auc_val: 0.8541                                                                                                    \n",
      "Epoch 5/20\n",
      "7414/7414 [==============================] - 185s 25ms/step - loss: 0.3413\n",
      "roc-auc: 0.8737 - roc-auc_val: 0.8644                                                                                                    \n",
      "Epoch 6/20\n",
      "7414/7414 [==============================] - 177s 24ms/step - loss: 0.3395\n",
      "roc-auc: 0.8766 - roc-auc_val: 0.8656                                                                                                    \n",
      "Epoch 7/20\n",
      "7414/7414 [==============================] - 176s 24ms/step - loss: 0.3378\n",
      "roc-auc: 0.8783 - roc-auc_val: 0.8659                                                                                                    \n",
      "Epoch 8/20\n",
      "7414/7414 [==============================] - 176s 24ms/step - loss: 0.3364\n",
      "roc-auc: 0.8803 - roc-auc_val: 0.8673                                                                                                    \n",
      "Epoch 9/20\n",
      "7414/7414 [==============================] - 176s 24ms/step - loss: 0.3353\n",
      "roc-auc: 0.8701 - roc-auc_val: 0.8559                                                                                                    \n",
      "Epoch 10/20\n",
      "7414/7414 [==============================] - 176s 24ms/step - loss: 0.3341\n",
      "roc-auc: 0.8846 - roc-auc_val: 0.8689                                                                                                    \n",
      "Epoch 11/20\n",
      "7414/7414 [==============================] - 178s 24ms/step - loss: 0.3332\n",
      "roc-auc: 0.8852 - roc-auc_val: 0.8685                                                                                                    \n",
      "Epoch 12/20\n",
      "7414/7414 [==============================] - 182s 24ms/step - loss: 0.3322\n",
      "roc-auc: 0.8868 - roc-auc_val: 0.8688                                                                                                    \n",
      "Epoch 13/20\n",
      "7414/7414 [==============================] - 181s 24ms/step - loss: 0.3312\n",
      "roc-auc: 0.8875 - roc-auc_val: 0.8692                                                                                                    \n",
      "Epoch 14/20\n",
      "7414/7414 [==============================] - 177s 24ms/step - loss: 0.3306\n",
      "roc-auc: 0.8853 - roc-auc_val: 0.8652                                                                                                    \n",
      "Epoch 15/20\n",
      "7414/7414 [==============================] - 181s 24ms/step - loss: 0.3296\n",
      "roc-auc: 0.8889 - roc-auc_val: 0.8687                                                                                                    \n",
      "Epoch 16/20\n",
      "7414/7414 [==============================] - 178s 24ms/step - loss: 0.3288\n",
      "roc-auc: 0.8624 - roc-auc_val: 0.8425                                                                                                    \n",
      "Epoch 17/20\n",
      "7414/7414 [==============================] - 172s 23ms/step - loss: 0.3280\n",
      "roc-auc: 0.8887 - roc-auc_val: 0.8658                                                                                                    \n",
      "Epoch 18/20\n",
      "7414/7414 [==============================] - 179s 24ms/step - loss: 0.3273\n",
      "roc-auc: 0.8803 - roc-auc_val: 0.8585                                                                                                    \n",
      "Epoch 19/20\n",
      "7414/7414 [==============================] - 178s 24ms/step - loss: 0.3267\n",
      "roc-auc: 0.8859 - roc-auc_val: 0.8631                                                                                                    \n",
      "Epoch 20/20\n",
      "7414/7414 [==============================] - 182s 24ms/step - loss: 0.3259\n",
      "roc-auc: 0.8801 - roc-auc_val: 0.8572                                                                                                    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fold 1\n",
      "Epoch 1/20\n",
      "7414/7414 [==============================] - 194s 26ms/step - loss: 0.3739\n",
      "roc-auc: 0.856 - roc-auc_val: 0.8545                                                                                                    \n",
      "Epoch 2/20\n",
      "7414/7414 [==============================] - 191s 26ms/step - loss: 0.3518\n",
      "roc-auc: 0.8538 - roc-auc_val: 0.8509                                                                                                    \n",
      "Epoch 3/20\n",
      "7414/7414 [==============================] - 192s 26ms/step - loss: 0.3469\n",
      "roc-auc: 0.867 - roc-auc_val: 0.8625                                                                                                    \n",
      "Epoch 4/20\n",
      "7414/7414 [==============================] - 179s 24ms/step - loss: 0.3438\n",
      "roc-auc: 0.8732 - roc-auc_val: 0.8663                                                                                                    \n",
      "Epoch 5/20\n",
      "7414/7414 [==============================] - 170s 23ms/step - loss: 0.3415\n",
      "roc-auc: 0.8726 - roc-auc_val: 0.8642                                                                                                    \n",
      "Epoch 6/20\n",
      "7414/7414 [==============================] - 171s 23ms/step - loss: 0.3395\n",
      "roc-auc: 0.8583 - roc-auc_val: 0.8484                                                                                                    \n",
      "Epoch 7/20\n",
      "7414/7414 [==============================] - 178s 24ms/step - loss: 0.3377\n",
      "roc-auc: 0.8797 - roc-auc_val: 0.8677                                                                                                    \n",
      "Epoch 8/20\n",
      "7414/7414 [==============================] - 185s 25ms/step - loss: 0.3363\n",
      "roc-auc: 0.8777 - roc-auc_val: 0.8644                                                                                                    \n",
      "Epoch 9/20\n",
      "7414/7414 [==============================] - 176s 24ms/step - loss: 0.3350\n",
      "roc-auc: 0.8676 - roc-auc_val: 0.8539                                                                                                    \n",
      "Epoch 10/20\n",
      "7414/7414 [==============================] - 173s 23ms/step - loss: 0.3339\n",
      "roc-auc: 0.867 - roc-auc_val: 0.8521                                                                                                    \n",
      "Epoch 11/20\n",
      "7414/7414 [==============================] - 171s 23ms/step - loss: 0.3328\n",
      "roc-auc: 0.885 - roc-auc_val: 0.868                                                                                                    \n",
      "Epoch 12/20\n",
      "7414/7414 [==============================] - 172s 23ms/step - loss: 0.3318\n",
      "roc-auc: 0.88 - roc-auc_val: 0.8625                                                                                                    \n",
      "Epoch 13/20\n",
      "7414/7414 [==============================] - 171s 23ms/step - loss: 0.3312\n",
      "roc-auc: 0.8736 - roc-auc_val: 0.8552                                                                                                    \n",
      "Epoch 14/20\n",
      "7414/7414 [==============================] - 171s 23ms/step - loss: 0.3303\n",
      "roc-auc: 0.8854 - roc-auc_val: 0.8662                                                                                                    \n",
      "Epoch 15/20\n",
      "7414/7414 [==============================] - 171s 23ms/step - loss: 0.3295\n",
      "roc-auc: 0.8836 - roc-auc_val: 0.8638                                                                                                    \n",
      "Epoch 16/20\n",
      "7414/7414 [==============================] - 186s 25ms/step - loss: 0.3287\n",
      "roc-auc: 0.8784 - roc-auc_val: 0.8571                                                                                                    \n",
      "Epoch 17/20\n",
      "7414/7414 [==============================] - 216s 29ms/step - loss: 0.3282\n",
      "roc-auc: 0.8876 - roc-auc_val: 0.8659                                                                                                    \n",
      "Epoch 18/20\n",
      "7414/7414 [==============================] - 212s 29ms/step - loss: 0.3273\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "test_pred = np.zeros((len(test_x), 1))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for i, (tr_idx, va_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "    print('-'*100)\n",
    "    print('Fold %d' % i)\n",
    "    trn_x, trn_y = train_x.iloc[tr_idx], train_y[tr_idx]\n",
    "    val_x, val_y = train_x.iloc[va_idx], train_y[va_idx]\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = DNN(single_embed_cnt, len(num_feat), embed_size=32)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',)\n",
    "    trn_generator = DataSequence(trn_x, trn_y, cat_feat, num_feat, batch_size=BATCH_SIZE)\n",
    "    val_generator = DataSequence(val_x, val_y, cat_feat, num_feat, batch_size=BATCH_SIZE)\n",
    "    test_generator = DataSequence(test_x, np.zeros(len(test_x)), cat_feat, num_feat, batch_size=BATCH_SIZE)\n",
    "    history = model.fit_generator(generator=trn_generator, \n",
    "                        epochs=20, \n",
    "                        verbose=1, \n",
    "                        callbacks=[MetricsCallback(trn_x, trn_y, val_x, val_y, cat_feat, num_feat, \n",
    "                                                   batch_size=BATCH_SIZE*4,\n",
    "                                                   save_name='main_dnn_f%d.h5' % i)], \n",
    "                        max_queue_size=10, \n",
    "                        workers=1, \n",
    "                        use_multiprocessing=False)\n",
    "    \n",
    "    model.load_weights(os.path.join(WEIGHT_PATH, 'main_dnn_f%d.h5' % i))\n",
    "    \n",
    "    test_pred += model.predict_generator(test_generator) / 5\n",
    "    \n",
    "    del trn_x, trn_y, val_x, val_y\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del trn_x, trn_y, val_x, val_y\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data/data_set_0926'\n",
    "invite_info_evaluate = pd.read_csv(os.path.join(PATH, 'invite_info_evaluate_1_0926.txt'), \n",
    "                          names=['question_id', 'author_id', 'invite_time'], sep='\\t')\n",
    "result = invite_info_evaluate\n",
    "result['result'] = test_pred \n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localtime = time.localtime(time.time())\n",
    "save_path = './out/result_dnn_%02d%02d%02d%02d.txt' % (localtime[1], localtime[2], localtime[3], localtime[4])\n",
    "result.to_csv(save_path, sep='\\t', index=False, header=False)\n",
    "print('%s saved.' % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.874397  0.849049237632428\n",
    "# 0.880489  0.844101158553199 -\n",
    "# 0.878943  0.851078697864456\n",
    "# 0.880510  0.854421755061172\n",
    "# 0.885294  0.858160387338417\n",
    "# 0.885622  0.857313282888585 -\n",
    "# 0.886196  0.856...          -\n",
    "# 0.887172  0.863096415680472\n",
    "# 0.888104  0.862779338260129\n",
    "# 0.888148  0.862893038464606\n",
    "# 0.888559  0.863349443045746\n",
    "# 0.888572  0.864079617350822\n",
    "# 0.888655  0.863959534391522\n",
    "# 0.888649  0.863831492125684\n",
    "# 0.889573  0.867511284503794\n",
    "# 0.890168  0.868963718646371 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
